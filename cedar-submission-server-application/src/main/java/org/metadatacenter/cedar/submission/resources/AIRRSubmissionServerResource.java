package org.metadatacenter.cedar.submission.resources;

import com.codahale.metrics.annotation.Timed;
import com.google.common.io.Files;
import org.apache.commons.fileupload.FileItem;
import org.apache.commons.fileupload.FileUploadException;
import org.apache.commons.fileupload.disk.DiskFileItemFactory;
import org.apache.commons.fileupload.servlet.ServletFileUpload;
import org.apache.commons.io.IOUtils;
import org.apache.commons.net.ftp.FTPClient;
import org.apache.commons.net.ftp.FTPReply;
import org.metadatacenter.cedar.submission.util.fileupload.flow.FlowChunkData;
import org.metadatacenter.cedar.submission.util.fileupload.flow.FlowChunkUploadManager;
import org.metadatacenter.cedar.submission.util.fileupload.flow.FlowUploadUtil;
import org.metadatacenter.cedar.util.dw.CedarMicroserviceResource;
import org.metadatacenter.config.CedarConfig;
import org.metadatacenter.config.FTPConfig;
import org.metadatacenter.exception.CedarException;
import org.metadatacenter.rest.context.CedarRequestContext;
import org.metadatacenter.rest.context.CedarRequestContextFactory;
import org.metadatacenter.submission.AIRRTemplate2SRAConverter;
import org.metadatacenter.submission.BioSampleValidator;
import org.metadatacenter.submission.biosample.AIRRTemplate;
import org.metadatacenter.util.json.JsonMapper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import javax.management.InstanceNotFoundException;
import javax.ws.rs.Consumes;
import javax.ws.rs.POST;
import javax.ws.rs.Path;
import javax.ws.rs.Produces;
import javax.ws.rs.core.MediaType;
import javax.ws.rs.core.Response;
import javax.xml.bind.JAXBException;
import javax.xml.datatype.DatatypeConfigurationException;
import java.io.*;
import java.net.SocketException;
import java.util.List;
import java.util.Optional;

import static org.metadatacenter.rest.assertion.GenericAssertions.LoggedIn;

/**
 * See here for submission instructions to NCBI:
 * <p>
 * https://docs.google.com/document/d/1tmPinCgaTwBkTsOwjitquFc0ZUN65w5xZs30q5phRkY/edit
 */
@Path("/command")
@Produces(MediaType.APPLICATION_JSON)
public class AIRRSubmissionServerResource
    extends CedarMicroserviceResource {
  final static Logger logger = LoggerFactory.getLogger(AIRRSubmissionServerResource.class);

  private final BioSampleValidator bioSampleValidator;

  private final AIRRTemplate2SRAConverter airrTemplate2SRAConverter;

  public AIRRSubmissionServerResource(CedarConfig cedarConfig) {
    super(cedarConfig);
    this.bioSampleValidator = new BioSampleValidator();
    this.airrTemplate2SRAConverter = new AIRRTemplate2SRAConverter();
  }

  /**
   * The {@link AIRRTemplate} class is generated by jsonschema2pojo from the
   * AIRRTemplate.json JSON Schema file in the resources directory. This file
   * contains the CEDAR template that defines the example SRA submission generated from an AIRR template.
   * An SRA submission incorporates BioSample metadata and a BioProject data.
   *
   * @param airrInstance An instance of an AIRR template
   * @return A validation response
   */
  @POST
  @Timed
  @Path("/validate-airr")
  @Consumes(MediaType.APPLICATION_JSON)
  public Response validate(
      AIRRTemplate airrInstance) throws CedarException {

    CedarRequestContext c = CedarRequestContextFactory.fromRequest(request);
    c.must(c.user()).be(LoggedIn);

    try {
      String bioSampleSubmissionXML = this.airrTemplate2SRAConverter
          .generateSRASubmissionXMLFromAIRRTemplateInstance(airrInstance);

      return Response.ok(this.bioSampleValidator.validateBioSampleSubmission(bioSampleSubmissionXML)).build();
    } catch (JAXBException | DatatypeConfigurationException e) {
      return Response.status(Response.Status.INTERNAL_SERVER_ERROR).build();
    }
  }

  @POST
  @Timed
  @Path("/submit-airr")
  @Consumes(MediaType.MULTIPART_FORM_DATA)
  public Response submitAIRR()
      throws CedarException {
    Optional<FTPClient> ftpClient = createFTPClient(cedarConfig.getSubmissionConfig().getNcbi().getSra().getFtp());
    String ftpHost = cedarConfig.getSubmissionConfig().getNcbi().getSra().getFtp().getHost();

    CedarRequestContext c = CedarRequestContextFactory.fromRequest(request);
    c.must(c.user()).be(LoggedIn);

    try {
      if (ServletFileUpload.isMultipartContent(request)) {
        File tempDir = Files.createTempDir();
        List<FileItem> fileItems = new ServletFileUpload(new DiskFileItemFactory(1024 * 1024, tempDir)).
            parseRequest(request);

        if (ftpClient.isPresent()) {
          for (FileItem fileItem : fileItems) {
            if (!fileItem.isFormField()) {
              String fieldName = fileItem.getFieldName();
              if ("instance".equals(fieldName)) { // This is the AIRR instance JSON
                InputStream is = fileItem.getInputStream();
                AIRRTemplate airrInstance = JsonMapper.MAPPER.readValue(is, AIRRTemplate.class);
                String bioSampleSubmissionXML = this.airrTemplate2SRAConverter
                    .generateSRASubmissionXMLFromAIRRTemplateInstance(airrInstance);
                InputStream xmlStream = IOUtils.toInputStream(bioSampleSubmissionXML);
                logger.info("Uploading submission XML");
                ftpClient.get().storeFile("submission.xml", xmlStream);
                is.close();
              } else { // The user-supplied files
                InputStream is = fileItem.getInputStream();
                String fileName = fileItem.getName();
                logger.info("Uploading user-supplied data file " + fileName);
                ftpClient.get().storeFile(fileName, is);
                is.close();
              }
            }
          }
        } else {
          logger.warn("Failed to connect to FTP host " + ftpHost);
          return Response.status(Response.Status.INTERNAL_SERVER_ERROR).build();
        }
      }
      return Response.ok().build();
    } catch (IOException e) {
      logger.warn("IO exception connecting to host " + ftpHost + ": " + e.getMessage());
      return Response.status(Response.Status.INTERNAL_SERVER_ERROR).build();
    } catch (FileUploadException e) {
      logger.warn("File upload exception uploading to host " + ftpHost + ": " + e.getMessage());
      return Response.status(Response.Status.INTERNAL_SERVER_ERROR).build();
    } catch (JAXBException e) {
      logger.warn("JAXB exception extracting AIRR submission" + e.getMessage());
      return Response.status(Response.Status.INTERNAL_SERVER_ERROR).build();
    } catch (DatatypeConfigurationException e) {
      logger.warn("Datatype configuration exception extracting AIRR submission" + e.getMessage());
      return Response.status(Response.Status.INTERNAL_SERVER_ERROR).build();
    } finally {
      try {
        if (ftpClient.isPresent()) {
          ftpClient.get().disconnect();
        }
      } catch (IOException e) {
        logger.warn("Exception disconnecting from FTP host " + ftpHost + ": " + e.getMessage());
      }
    }
  }

  public Optional<FTPClient> createFTPClient(FTPConfig ftpConfig) {
    FTPClient ftpClient = new FTPClient();

    try {
      ftpClient.connect(ftpConfig.getHost());

      if (!ftpClient.login(ftpConfig.getUser(), ftpConfig.getPassword())) {
        ftpClient.logout();
        logger.warn("Failure logging in to FTP host " + ftpConfig.getHost());
        return Optional.empty();
      } else {
        int reply = ftpClient.getReplyCode();
        if (!FTPReply.isPositiveCompletion(reply)) {
          ftpClient.disconnect();
          logger.warn("Failed to connect to FTP host " + ftpConfig.getHost() + ", reply = " + reply);
          return Optional.empty();
        } else {
          ftpClient.enterLocalPassiveMode();
          ftpClient.changeWorkingDirectory(ftpConfig.getSubmissionDirectory());
          logger.info("Connected to FTP host " + ftpConfig.getHost() + "; current directory is " + ftpClient
              .printWorkingDirectory());
          return Optional.of(ftpClient);
        }
      }
    } catch (SocketException e) {
      logger.warn("Socket exception connecting to FTP host: " + e.getMessage());
      return Optional.empty();
    } catch (IOException e) {
      logger.warn("IO exception connecting to FTP host: " + e.getMessage());
      return Optional.empty();
    }
  }

  /**
   * This endpoint receives multiple chunks of a file and assemblies them. When the upload is complete, it triggers
   * the FTP upload to the NCBI.
   * It is based on some ideas from: https://github.com/flowjs/flow.js/blob/master/samples/java/src/resumable/js/upload/UploadServlet.java
   * TODO: deal with multi-file submissions
   */
  @POST
  @Timed
  @Path("/upload-airr-to-cedar")
  @Consumes(MediaType.MULTIPART_FORM_DATA)
  public Response uploadAIRRToCEDAR() throws CedarException {

    CedarRequestContext c = CedarRequestContextFactory.fromRequest(request);
    c.must(c.user()).be(LoggedIn);

    // Check that this is a file upload request
    if (ServletFileUpload.isMultipartContent(request)) {
      try {
        List<FileItem> fileItems = new ServletFileUpload(new DiskFileItemFactory()).parseRequest(request);
        FlowChunkData info = FlowUploadUtil.getFlowChunkData(fileItems);
        RandomAccessFile raf = null;
        try {
          File uploadedFile = File.createTempFile(FlowUploadUtil.getFileNamePrefix(info.flowFilename),
              FlowUploadUtil.getFileNameSuffix(info.flowFilename));

          // The file will be deleted when the virtual machine terminates. We will also call uploadedFile.delete() when
          // the upload to the NCBI is complete. If for some reason the upload fails, the file will be deleted when
          // the VM terminates.
          uploadedFile.deleteOnExit();

          raf = new RandomAccessFile(uploadedFile, "rw");

          // Seek to position
          raf.seek((info.flowChunkNumber - 1) * info.flowChunkSize);
          // Save to file
          InputStream is = info.getFlowFileInputStream();
          long read = 0;
          long content_length = request.getContentLength();
          byte[] bytes = new byte[1024 * 100];
          while (read < content_length) {
            int r = is.read(bytes);
            if (r < 0) {
              break;
            }
            raf.write(bytes, 0, r);
            read += r;
          }
          raf.close();

          // Update the map
          FlowChunkUploadManager.getInstance().increaseUploadedChunksCount(info.getFlowIdentifier(), info.getFlowTotalChunks());

          // Check if the upload is complete and trigger the FTP submission to NCBI
          if (FlowChunkUploadManager.getInstance().isUploadFinished(info.getFlowIdentifier())) {
            FlowChunkUploadManager.getInstance().removeFlowStatus(info.getFlowIdentifier());
            System.out.println("UPLOAD COMPLETED!: " + info.getFlowIdentifier());
            System.out.println(FlowChunkUploadManager.getInstance().toString());
          }

          // Check if uploaded and trigger the FTP upload to the NCBI
//          info.uploadedChunks.add(new ResumableInfo.ResumableChunkNumber(resumableChunkNumber));
//          if (info.checkIfUploadFinished()) { //Check if all chunks uploaded, and change filename
//            ResumableInfoStorage.getInstance().remove(info);
//            response.getWriter().print("All finished.");
//          } else {
//            response.getWriter().print("Upload");
//          }

        } catch (FileNotFoundException e) {
          e.printStackTrace();
        } catch (IOException e) {
          e.printStackTrace();
        } catch (InstanceNotFoundException e) {
          e.printStackTrace();
        }
      } catch (FileUploadException e) {
        e.printStackTrace();
      }
      return Response.ok().build();
    } else {
      return Response.status(Response.Status.BAD_REQUEST).build();
    }
  }

}
